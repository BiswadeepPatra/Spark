##### /home/cloudera/example-1.conf
# Name the components on this agent
# I define two sources:
# a source for access log file
# a source for for error log file
agent1.sources = tailAccessSource
# I define one sink
agent1.sinks = hdfsSink01  hdfsSink02
# I define one channel
agent1.channels = memChannel01 memChannel02
 
# Bind the source and sink to the channel
# Both sources will use the memory channel 
agent1.sources.tailAccessSource.channels = memChannel01 memChannel02

agent1.sources.tailAccessSource.sector.type = replicating
agent1.sources.tailAccessSource.sector.optional = memChannel02

agent1.sinks.hdfsSink01.channel = memChannel01
agent1.sinks.hdfsSink02.channel = memChannel02 
 
agent1.sources.tailAccessSource.type = spooldir
agent1.sources.tailAccessSource.spoolDir = /tmp/spooldir/
 
# Define the type and options for the sink
# Note: namenode is the hostname the hadoop namenode server
# flume/data-example.1/ is the directory where the apache logs will be stored
#agent1.sinks.hdfsSink.channel = memory-channel
agent1.sinks.hdfsSink01.type = hdfs
agent1.sinks.hdfsSink01.hdfs.path = /tmp/flume/primary
agent1.sinks.hdfsSink01.hdfs.filePrefix = events
agent1.sinks.hdfsSink01.hdfs.fileSuffix = .log
agent1.sinks.hdfsSink01.hdfs.inUsePrefix = _
agent1.sinks.hdfsSink01.hdfs.fileType = DataStream

agent1.sinks.hdfsSink02.type = hdfs
agent1.sinks.hdfsSink02.hdfs.path = /tmp/flume/primary
agent1.sinks.hdfsSink02.hdfs.filePrefix = events
agent1.sinks.hdfsSink02.hdfs.fileSuffix = .log
agent1.sinks.hdfsSink02.hdfs.inUsePrefix = _
agent1.sinks.hdfsSink02.hdfs.fileType = DataStream

agent1.channels.memChannel01.type = file
agent1.channels.memChannel02.type = memory