##### /home/cloudera/example-1.conf
# Name the components on this agent
# I define two sources:
# a source for access log file
# a source for for error log file
agent1.sources = tailAccessSource
# I define one sink
agent1.sinks = hdfsSink
# I define one channel
agent1.channels = memChannel01
 
# Bind the source and sink to the channel
# Both sources will use the memory channel 
agent1.sources.tailAccessSource.channels = memChannel01
agent1.sinks.hdfsSink.channel = memChannel01
 
 
# Define the type and options for each sources
agent1.sources.tailAccessSource.interceptor = i1
agent1.sources.tailAccessSource.interceptor.i1.type = timestamp
agent1.sources.tailAccessSource.interceptor.i1.preserveExisting = true

# Define the type and options for each sources
agent1.sources.tailAccessSource.type = exec
agent1.sources.tailAccessSource.command = tail -F /var/log/httpd/access_log

# Define the type and options for the channel
agent1.channels.memChannel01.type = memory
agent1.channels.memChannel01.capacity = 1000
agent1.channels.memChannel01.transactionCapacity = 100
 
 
# Define the type and options for the sink
# Note: namenode is the hostname the hadoop namenode server
# flume/data-example.1/ is the directory where the apache logs will be stored
agent1.sinks.hdfsSink.channel = memChannel01
agent1.sinks.hdfsSink.type = hdfs
agent1.sinks.hdfsSink.hdfs.path = hdfs://quickstart.cloudera:8020/user/cloudera/flume/
#agent1.sinks.hdfsSink.hdfs.path = hdfs://namenode/flume/%Y/%m/%d/%H/%M
agent1.sinks.hdfsSink.hdfs.fileType = DataStream
#agent1.sinks.hdfsSink.hdfs.rollCount = 0
#agent1.sinks.hdfsSink.hdfs.rollSize = 0
#agent1.sinks.hdfsSink.hdfs.rollInterval = 60